{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dc77b8",
   "metadata": {},
   "source": [
    "### Which models performed best on smaller vs. larger datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1946701",
   "metadata": {},
   "source": [
    "On the small, eight-feature diabetes set, it appears non-linear methods edged out the rest: kernel SVM showed the highest average F1 under cross-validation, and decision trees delivered the strongest minority class (diabetes-positive) F1 on the held-out split. By contrast, on the comparably large cancer dataset (with 30 features!!), virtually every linear-model-including logistic regression and linear SVM showed incredible cross-validated F1 scores, matching or exceeding their kernel-based counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a257666",
   "metadata": {},
   "source": [
    "### How different models handle imbalanced data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75af21",
   "metadata": {},
   "source": [
    "Imbalanced classes showed differences in each algorithmâ€™s tendencies: Naive Bayes consistently favored recall over precision, catching more true positives at the cost of false ones, while decision trees had the best balance between precision and recall on both datasets. SVMs and logistic regression sat in between, offering moderate precision and recall without strongly skewing toward one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27909719",
   "metadata": {},
   "source": [
    "### Compare results from using train/test split and using cross-fold validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6539b9",
   "metadata": {},
   "source": [
    "Across both dataset problems, cross-fold validation consistently yielded more stable, conservative F1 estimates. Whereas single train/test splits sometimes over or under estimated performance by a few points (usually with decision trees which showed higher variance between CV and test-split results). In practice, CV gives a reliable baseline, but it's wise to verify performance on a holdout set especially for high-variance learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1eaba",
   "metadata": {},
   "source": [
    "### Compare and contrast results from the primary dataset (diabetes.csv) and the secondary dataset (cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3003a6",
   "metadata": {},
   "source": [
    "Compraing the two datasets, the simpler diabetes data (fewer features, noisier signals) capped model performance around the mid-60s F1 range, leaving room for non-linear or tree-based methods to eke out modest gains. The richer, higher-dimensional cancer data proved far more seperable, letting even basic linear models hit the high-90s. In short, small, noisy datasets may benefit most from kernel tricks or trees, whereas well-structured, high-dimensional data often let linear classifiers dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efd8d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
